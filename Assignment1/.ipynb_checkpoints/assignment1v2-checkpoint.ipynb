{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# matplotlib options\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib options\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Temporal Patterns\n",
    "\n",
    "In this exercise we will focuse on patterns over time for the 14 ``focuscrimes``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "focuscrimes = set(['WEAPON LAWS', 'PROSTITUTION', 'DRIVING UNDER THE INFLUENCE', 'ROBBERY', 'BURGLARY', 'ASSAULT', 'DRUNKENNESS', 'DRUG/NARCOTIC', 'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'VEHICLE THEFT', 'STOLEN PROPERTY', 'DISORDERLY CONDUCT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv' does not exist: b'Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-53306a3cb728>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Import the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv' does not exist: b'Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv'"
     ]
    }
   ],
   "source": [
    "## Import the data\n",
    "df = pd.read_csv(\"Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly patterns \n",
    "\n",
    "Initially we will look at the patterns over a weekly basis, from monday to sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with only the focus crimes\n",
    "temp = df[df['Category'].isin(focuscrimes)]\n",
    "# groupby category and day of week\n",
    "temp = temp.groupby(['Category','DayOfWeek']).count().X.unstack()\n",
    "# Arrange the dataframe so the weekday match the order of the week\n",
    "temp = temp[['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define names of crimes\n",
    "crimes = temp.index\n",
    "# Figure size\n",
    "plt.rcParams['figure.figsize'] = (15, 20)\n",
    "# Initialize the figure\n",
    "fig, axs = plt.subplots(7, 2, sharex='col')\n",
    "p = 0\n",
    "\n",
    "for i in range(0,7):\n",
    "    for j in range(0,2):\n",
    "        axs[i,j].bar(temp.loc[crimes[p]].index,temp.loc[crimes[p]].values)\n",
    "        axs[i,j].set_title(crimes[p])\n",
    "        axs[i,j].set_ylabel('Observations [#]')\n",
    "        p +=1\n",
    "\n",
    "# Specify x axis label\n",
    "axs[6,0].set_xlabel('Weekday')\n",
    "axs[6,1].set_xlabel('Weekday')\n",
    "# Title for subplot\n",
    "plt.suptitle('Weekly patterns for the focuscrimes', size = 24)\n",
    "# Ensures a nice and thight layout\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot several interesting patterns can be seen.\n",
    "- ``DRIVING UNDER THE INFLUENCE`` and ``DRUNKENNESS`` peaks during the weekend which is as expected.\n",
    "- ``BURGLARY`` has its peak during friday which might be because many people go out on fridays and many homes are left empty.\n",
    "- ``PROSTITUTION`` peaks during mid week which is rather unexpected.\n",
    "- ``DRUG/NARCOTIC`` peaks during wednesday and is at its lowest during the weekend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The months (yearly patterns)\n",
    "To check if some months are worse than others, the yearly pattern is plotted for all focuscrimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date and time to datetime\n",
    "df['Datetime'] = pd.to_datetime(df['Date']+ ' ' + df['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column specifying the \n",
    "df['month'] = df['Datetime'].dt.month\n",
    "# Dataframe with only the focus crimes\n",
    "temp = df[df['Category'].isin(focuscrimes)]\n",
    "# Group the data by category and month\n",
    "temp = temp.groupby(['Category','month']).count().X.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define names of crimes\n",
    "crimes = temp.index\n",
    "# Figure size\n",
    "plt.rcParams['figure.figsize'] = (15, 20)\n",
    "# Initialize the figure\n",
    "fig, axs = plt.subplots(7, 2, sharex='col')\n",
    "p = 0\n",
    "\n",
    "for i in range(0,7):\n",
    "    for j in range(0,2):\n",
    "        axs[i,j].bar(temp.loc[crimes[p]].index,temp.loc[crimes[p]].values)\n",
    "        axs[i,j].set_title(crimes[p])\n",
    "        axs[i,j].set_ylabel('Observations [#]')\n",
    "        p +=1\n",
    "        \n",
    "# Specify x axis label\n",
    "axs[6,0].set_xlabel('Month')\n",
    "axs[6,1].set_xlabel('Month')\n",
    "# Title for subplot\n",
    "plt.suptitle('Yearly pattern for the focuscrimes', size = 24)\n",
    "# Ensures a nice and thight layout\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots it can be seen that most crimes appear to be at around the same level through out the year. Some categories like ``DRUG/NARCOTIC`` do however have fewer crimes during the summer months and december. This could be due to people being on holidays and thereby generally fewer people around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24 hour cycle \n",
    "To get an idea of if a crime happens during a specific time of the day, the crimes are plotted over a 24 hour cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column with the recorded hour of the crime\n",
    "df['hour'] = df['Datetime'].dt.hour\n",
    "# Dataframe with only the focus crimes\n",
    "temp = df[df['Category'].isin(focuscrimes)]\n",
    "# Group the data by category and month\n",
    "temp = temp.groupby(['Category','hour']).count().X.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define names of crimes\n",
    "crimes = temp.index\n",
    "# Figure size\n",
    "plt.rcParams['figure.figsize'] = (15, 20)\n",
    "# Initialize the figure\n",
    "fig, axs = plt.subplots(7, 2, sharex='col')\n",
    "p = 0\n",
    "\n",
    "for i in range(0,7):\n",
    "    for j in range(0,2):\n",
    "        axs[i,j].bar(temp.loc[crimes[p]].index+1,temp.loc[crimes[p]].values) # add 1 so it matches the real hour\n",
    "        axs[i,j].set_title(crimes[p])\n",
    "        axs[i,j].set_ylabel('Observations [#]')\n",
    "        p +=1\n",
    "        \n",
    "# Specify x axis label\n",
    "axs[6,0].set_xlabel('Hour of day')\n",
    "axs[6,1].set_xlabel('Hour of day')\n",
    "# Title for subplot\n",
    "plt.suptitle('24 hour patterns for the focuscrimes', size = 24)\n",
    "# Ensures a nice and thight layout\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots it can be seen that the pattern on a 24 hour cycle are very different depending on the crime type. Some of the interesting patterns are described below.\n",
    "- For some of the categories such as ``DRIVING UNDER THE INFLUENCE``, ``DRUNKENNESS`` and ``PROSTITUTION`` most of the crimes happen the recorded crimes happens during the night.\n",
    "- For the categories like ``DRUG/NARCOTIC``and ``LARCENY/THEFT`` it can be seen that most of the crimes happens during the day hours.\n",
    "- For categories like ``TREPASS`` and ``DISORDERLY CONDUCT`` the peak amount of crimes is in the morning whereafter less crimes appear through out the day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hours of the week \n",
    "Lastly it is interesting to see if the 24 hour cycle changes chrough out the week. Therefor all crimes are plotted for the 168 hours of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[df['Category'].isin(focuscrimes)] #creating a dataframe that only contains focus crimes \n",
    "temp = temp.groupby(['Category', 'hour','DayOfWeek']).size() #group by category and year_of_crime\n",
    "temp = temp.unstack().unstack() #using unstack a couple of times so it will be turned to the correct dataframe\n",
    "temp = temp[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']]\n",
    "temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure size\n",
    "plt.rcParams['figure.figsize'] = (15, 20)\n",
    "\n",
    "# Initialize the figure\n",
    "fig, axs = plt.subplots(7, 2, sharex='col')\n",
    "ind = temp.index \n",
    "p = 0\n",
    "for i in range(0,7):\n",
    "    for j in range(0,2):\n",
    "        axs[i,j].bar(range(0,len(temp.loc[ind[1]].index)),temp.loc[ind[p]].values);\n",
    "        axs[i,j].set_title(ind[p])\n",
    "        axs[i,j].set_ylabel('Observations [#]')\n",
    "        p+=1\n",
    "\n",
    "# Specify x axis label\n",
    "axs[6,0].set_xlabel('Hour of week')\n",
    "axs[6,1].set_xlabel('Hour of week')\n",
    "# Title for subplot\n",
    "plt.suptitle('24 hour patterns for the focuscrimes', size = 24)\n",
    "# Ensures a nice and thight layout\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above show similar patterns on a daily basis as the plots for a 24 hour cycle. However, the amount of observations differ through out the week for certain crimes, and some crimes have large spikes in observations. Below some of the interesting crimes are described.\n",
    "- ``DISORDERLY CONDUCT`` has a similar pattern through out the week, but it has some very large spikes from mondag to wednesday in the early hours of the day.\n",
    "- ``BURGLARY`` has large spikes for 2 hours during friday.\n",
    "- ``PROSTITUTION`` has some very large spikes during the mid part of the week. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Thinking about data and visualization\n",
    "### Questions for the first video\n",
    "\n",
    "#### What is the difference between data and metadata?\n",
    "Data can be some information or a list of observations where metadata is some informations about the data which helps us understand our data better. With regard to the bike-example we had our raw data, which was simply the *messy* datafile, and we add some context, some metadata, by looking at the bike route through for example google earth.\n",
    "\n",
    "#### Sune says that the human eye is a great tool for data analysis. Do you agree?\n",
    "The human eye is a great tool for data analysis as we use it every day for analysing everything we do. \n",
    "### The human eye is great at **HJÆLP!!** MENTION SOMEHTIN GTHAT IS DIFFICULT FOR THE HUMAN EYE!\n",
    "\n",
    "#### Example of Simpsons's paradox\n",
    "Let's say that you want to go out to get a good Dürum at Nørrebro with your friend. This can be very tricky as there allmost as many people as Dürum places within Nørrebro, so deciding on one can take ages. To make it easier you decide that both you and your friend should find a Dürum place with good reviews online and then you will simply pick the one with the best reviews, sounds easy right? \n",
    "\n",
    "You find a Dürum place called Dürumsymfonien which is recommended by a higher percentage of both men and woman than your friends Dürum place, Dürumbar. Just as you are about to lace up your shoes your friend, using the exact same data, finds that Dürumbar is recommended by a higher percentage of all users, so it is clearly the winner. As you are very hungry you decide to just eat a Dürum from both places and figure out what the heck is going on when you come back home with a full belly. \n",
    "\n",
    "Upon arriving home your realize that you had entered the Simpson's Paradox, where both Dürum places can be better and worse than eachother, where a dataset can be used to prove two opposing arguments. Below the reviews of the two Dürum places are showed.\n",
    "\n",
    "| | Dürumbar | Dürumsymfonien |\n",
    "|---|---|---|\n",
    "|Male | 30/100 = 30% |105/200 = 53% |\n",
    "|Female | 200/250 = 80% | 9/10 = 90% |\n",
    "|Combined | 230/350 = 66% | 114/210 = 54%|\n",
    "\n",
    "The problem appears as the amount of reviews is not accounted for. Dürumbar has a lot more reviews from females than Dürumsymfonien and Dürumsymfonien has a lot more reviews from males than Dürumbar.\n",
    "\n",
    "## hente MEGET inspiration herfra: https://towardsdatascience.com/simpsons-paradox-how-to-prove-two-opposite-arguments-using-one-dataset-1c9c917f5ff9 for meget?\n",
    "\n",
    "#### Difference between exploratory and explanatory data analysis\n",
    "Doing exploratory data analysis is all about structuring and visualizing the data to try to understand what data you have, and what interesting stories and evidence may lie hidden in the data. Explanatory data analysis on the other hand is about visualizing the data for an audience, showing what you found in the exploratory analysis in a understandable way. The difference can be seen as the difference between doing an presentation and all the hard work which lies behind it, where the presentation is the explanatory analysis and all the notes, calculations, litterature reading etc., which is only for yourself, is the exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Excercise:* Questions for the [first video lecture](https://www.youtube.com/watch?v=9D2aI30AMhM).\n",
    "* What is the difference between *data* and *metadata*? How does that relate to the bike-example?\n",
    "* Sune says that the human eye is a great tool for data analysis. Do you agree? Explain why/why not. Mention something that the human eye is very good at. Can you think of something that [is difficult for the human eye](http://cdn.ebaumsworld.com/mediaFiles/picture/718392/84732652.jpg). Explain why your example is difficult. \n",
    "* Simpson's paradox is hard to explain. Come up with your own example - or find one on line.\n",
    "* In your own words, explain the difference between *exploratory* and *explanatory* data analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for the second video lecture\n",
    "\n",
    "#### Explain how the Pearson correlation works and write down its mathematical formulation. Can you think of an example where it fails (and visualization works)?\n",
    "\n",
    "\n",
    "#### What is the difference between a bar-chart and a histogram?\n",
    "\n",
    "#### How do you find the right bin-size for a hisogram?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Excercise:* Questions for the [second video lecture](https://www.youtube.com/watch?v=yiU56codNlI).\n",
    "* As mentioned earlier, visualization is not the only way to test for correlation. We can (for example) calculate the Pearson correlation. Explain in your own words how the Pearson correlation works and write down it's mathematical formulation. Can you think of an example where it fails (and visualization works)?\n",
    "* What is the difference between a bar-chart and a histogram?\n",
    "* I mention in the video that it's important to choose the right bin-size in histograms. But how do you do that? Do a Google search to find a criterion you like and explain it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Generating important plot types\n",
    "\n",
    "*Excercise*: Let us recreate some plots from DAOST but using our own favorite dataset.\n",
    "\n",
    "* First, let's make a jitter-plot (that is, code up something like **Figure 2-1** from DAOST from scratch), but based on SF Police data. My hunch from inspecting the file is that the police-folks might be a little bit lazy in noting down the **exact** time down to the second. So choose a crime-type and a suitable time interval (somewhere between a month and 6 months depending on the crime-type) and create a jitter plot of the arrest times during a single hour (like 13-14, for example). So let time run on the $x$-axis and create vertical jitter.\n",
    "\n",
    "* Now for some histograms (please create a crime-data based versions of the plot-type shown in DAOST **Figure 2-2**). (I think the GPS data could be fun to understand from this perspective.) \n",
    "  * This time, pick two crime-types with different geographical patterns **and** a suitable time-interval for each (you want between 1000 and 10000 points in your histogram)\n",
    "  * Then take the latitude part of the GPS coordinates for each crime and bin the latitudes so that you have around 50 bins across the city of SF. You can use your favorite method for binning. I like `numpy.histogram`. This function gives you the counts and then you do your own plotting. \n",
    "* Next up is using the plot-type shown in **Figure 2-4** from DAOST, but with the data you used to create Figure 2.1. To create the kernel density plot, you can either use `gaussian_kde` from `scipy.stats` ([for an example, check out this stackoverflow post](https://stackoverflow.com/questions/4150171/how-to-create-a-density-plot-in-matplotlib)) or you can use [`seaborn.kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html).\n",
    "* Finally, grab 25 random timepoints from the dataset (of 1000-10000 original data) you've just plotted and create a version of Figure 2-4 based on the 25 data points. Does this shed light on why I think KDEs can bee misleading? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Heatmaps of geo-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import folium\n",
    "\n",
    "#This part of the notebook is computed with inspiration from: \n",
    "#https://www.kaggle.com/daveianhickey/how-to-folium-for-maps-heatmaps-time-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise: A new take on geospatial data using Folium.*\n",
    "\n",
    "We are now studying geospectral data by plotting raw data points and heatmaps.\n",
    "\n",
    "* First start by plotting a map of San Francisco with a nice tight zoom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_SF = folium.Map([37.773972, -122.431297],tiles = \"Stamen Toner\", zoom_start=13) #Generating the folium Map, \n",
    "                                                    # the map type can be changed to different styles: \n",
    "                                                    # tiles_list = [\"Stamen Terrain\", \"Stamen Toner\", \"Mapbox Bright\"]\n",
    "map_SF #display the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, use the the coordinates for SF City Hall 37.77919, -122.41914 to indicate its location on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.Marker([37.77919, -122.41914], popup='City Hall').add_to(map_SF) #pop-up marker is added to the map\n",
    "map_SF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point scatter plot:\n",
    "\n",
    "* Now, let's plot some more data (no need for popups this time). Select a couple of months of data for 'DRUG/NARCOTIC' and draw a little dot for each arrest for those two months. We can call this a kind of visualization a point scatter plot.\n",
    "\n",
    "The year 2017 is chosen, and the month February and March will be investigated in the below point scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv file\n",
    "data = pd.read_csv(\"Week1/Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv\") #import data\n",
    "data['Datetime']= pd.to_datetime((data['Date'] + ' ' + data['Time'])) #creating a datetime column\n",
    "df = data[data['Category'] == 'DRUG/NARCOTIC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting only to work with two months\n",
    "df_febmar_17=df[(df['Datetime']>='2017-02-01 00:00:00') & (df['Datetime']<'2017-04-01 00:00:00')]\n",
    "print('Number of observations in february and march 2017:', df_febmar_17.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the point scatter plot ontop of the map\n",
    "[folium.CircleMarker(\n",
    "                     location=[df_febmar_17['Y'].iloc[i] , df_febmar_17['X'].iloc[i]], #Given all relevant coordinates \n",
    "                     radius=1, #setting the radius of the marker to 1\n",
    "                     color='#0080bb') #selecting the prefered color of the datapoints\n",
    "                     .add_to(map_SF) #adding the points to the map\n",
    "                     for i in range(len(df_febmar_17))] # adding all the point by using a for-loop\n",
    "map_SF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO WE WANT THE CITYHALL ON THE MAP OR NOT?? /JESPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap:\n",
    "\n",
    "* To create your first heatmap, grab all arrests for the category 'SEX OFFENSES, NON FORCIBLE' across all time. Play with parameters to get plots you like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "map_SF = folium.Map([37.773972, -122.431297],tiles = \"Stamen Toner\", zoom_start=13)\n",
    "\n",
    "# Ensure you're handing it floats\n",
    "df['X'] = df['X'].astype(float)\n",
    "df['Y'] = df['Y'].astype(float)\n",
    "\n",
    "# Filter the DF for rows, then columns, then remove NaNs\n",
    "df_heat = data[data['Category'] == 'SEX OFFENSES, NON FORCIBLE'] # Reducing data size so it runs faster\n",
    "df_heat = df_heat[['Y', 'X']] \n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "data_heat = [[row['Y'],row['X']] for index, row in df_heat.iterrows()]\n",
    "\n",
    "#Plot it on the map\n",
    "#HeatMap(data_heat, radius=15, blur=24, max_zoom=18, min_opacity=2).add_to(map_SF) \n",
    "HeatMap(data_heat, radius=15, blur=15).add_to(map_SF) \n",
    "\n",
    "# Display the map\n",
    "map_SF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, comment on the differences between scatter plots and heatmaps.\n",
    "    * What can you see using the scatter-plots that you can't see using the heatmaps?\n",
    "    * And vice versa: what does the heatmaps help you see that's difficult to distinguish in the scatter-plots?\n",
    "\n",
    "> The scatter plot show the exact locations for the crimes, all crimes are weighted equally with the same size and color point. If you zoom in or out of the view scatter points will always stay the same size irrelevant of the view. However a heatmap will change in expression and coloring, as some observations may interact with eachother when zooming out.\n",
    "\n",
    "> The heatmap on the other hand, makes it easier to see when more observations are clustered close together. Instead of just adding another point on top of one another as in the scatter plot, the heatmap varying in color and size to esially illustrate a local epicenter for that specific crime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Play around with the various parameter for heatmaps. Comment on the effect on the various parameters for the heatmaps. How do they change the picture? Information is found in a list here: https://python-visualization.github.io/folium/plugins.html\n",
    "\n",
    "> All the parameters have been investigated, to see how the influence the picture. However there will in the following be commented on the parameters radius, max_zone and blur as the were used to create the heatmap above.\n",
    "\n",
    "> Radius: Influence how large an area, and how strong the coloring of an observation should be plotted on the map.\n",
    "\n",
    "> Max_zone: Deficult to see any hugh difference. HELP!!!!\n",
    "\n",
    "> Blur: Change the coloring of the observations. A value close to 1 will make the bounderis of an observation more strict (no faded boundary), where a high value will fade out the boundaries more slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SKAL NOK SLETTES. PRØVEDE AT PLOTTE I ET SUBPLOT\n",
    "\n",
    "rad = np.arange(10,40,10)\n",
    "max_z = np.arange(15,30,5)\n",
    "min_op = np.arange(1,10,4)\n",
    "print('Radius range:', rad)\n",
    "print('Max_zone range:', max_z)\n",
    "print('Min_opacity range:', min_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_SF_R1 = folium.Map([37.773972, -122.431297],tiles = \"Stamen Toner\", zoom_start=13)\n",
    "map_SF_R2 = folium.Map([37.773972, -122.431297],tiles = \"Stamen Toner\", zoom_start=13)\n",
    "map_SF_R3 = folium.Map([37.773972, -122.431297],tiles = \"Stamen Toner\", zoom_start=13)\n",
    "\n",
    "# Ensure you're handing it floats\n",
    "df['X'] = df['X'].astype(float)\n",
    "df['Y'] = df['Y'].astype(float)\n",
    "\n",
    "# Filter the DF for rows, then columns, then remove NaNs\n",
    "df_heat = data[data['Category'] == 'SEX OFFENSES, NON FORCIBLE'] # Reducing data size so it runs faster\n",
    "df_heat = df_heat[['Y', 'X']] \n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "data_heat = [[row['Y'],row['X']] for index, row in df_heat.iterrows()]\n",
    "\n",
    "#Plot it on the map\n",
    "HeatMap(data_heat, radius=10, blur=24, max_zoom=18, min_opacity=2).add_to(map_SF_R1) \n",
    "HeatMap(data_heat, radius=20, blur=24, max_zoom=18, min_opacity=2).add_to(map_SF_R2)\n",
    "HeatMap(data_heat, radius=25, blur=24, max_zoom=18, min_opacity=2).add_to(map_SF_R3)\n",
    "\n",
    "MAPS = [map_SF_R1, map_SF_R2, map_SF_R3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plot to see the changes in the parameters\n",
    "\n",
    "fig, axs = plt.subplots(1, 3 , figsize=(14,17)) #generate the subplot\n",
    "ind = ['Radius', 'Max_zone', 'Min_opacity']\n",
    "p = 0\n",
    "for i in range(0,1):\n",
    "    for j in range(0,3):\n",
    "        axs[i,j].plot(MAPS[p]);\n",
    "        axs[i,j].set_title(ind[p])\n",
    "        p+=1\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SKAL NOK SLETTES HER NED TIL!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detective skills\n",
    "\n",
    "# INSERT PICTURE\n",
    "\n",
    "* In that screenshot, I've (manually) highlighted a specific hotspot for this type of crime. Use your detective skills to find out what's going on in that building on the 800 block of Bryant street ... and explain in your own words.\n",
    "\n",
    "> By doing a detective google search on 'San Francisco bryant street' the second link was sending us directly to the Hall of Justice, more precis the Superior Court of California. It is placed on 850 Bryant St., which fits perfect to the place where a lot of crimes is happening. This could be because of the police force in San Francisco needs to place a location for the crime, and they could have the Court house as a dummy location, if nothing else is stated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<a href=https://www.sfsuperiorcourt.org/general-info/directions/hall-of-justice?__cf_chl_captcha_tk__=fcd1a9e036de37ba1426b1852bf947afd82bb0bf-1582849405-0-AT4IlX0ZXdzRnJfVHQbac5Swq1bZ5BHhaJxGEXYLew0WYPIGj3hpR0GnJ9qoHNIQl991lls97EwKSymh509lmbjDCqiC-an6ESBb3ifbtUWmU6zTfWv6GF53rXmtKgaARlEdkQkZCde-GKqUb-GZXK5SPXmwShC8ajwNhnVG3pDfujkQ5lqsBCO41pnynS_QG9iFfdbeC14qgbFas625nv7jGQD9LP6ibLiHY_CGDfISy6jzkkQV7LeuE5L7_NErMh4RxntPWT7ozGXr9jeaVdc01L2oo8Yh6HeTY9m0xJDmzZ3JVw8VSc_9c86GYr2Pl4xSHL-aYTo1BDkh-UDYktt6AuXxCFo82zdF2d1JSWN9T7PqLMOHI09x7XRum2hkT6mJJ39T7P5c0s-rvUDDQpVXVu3SsNUbvL2d_P94GEak7m25BKZ2jF-qq3EPzsnL8HPib31at6C19o-1D7hJA2e_AZmbzDKmdhkX8aUmNGW_n4YYeeNW15m1DRv0W_MWXQ>Check out the link here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap movies\n",
    "\n",
    "* Start by choosing your favorite crimetype. Prefereably one with spatial patterns that change over time (use your data-exploration from the previous lectures to choose a good one).\n",
    "* Now, choose a time-resolution. You could plot daily, weekly, monthly datasets to plot in your movie. Again the goal is to find interesting temporal patterns to display. We want at least 20 frames though.\n",
    "* Create the movie using HeatMapWithTime.\n",
    "* Comment on your results:\n",
    "    * What patterns does your movie reveal?\n",
    "    * Motivate/explain the reasoning behind your choice of crimetype and time-resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "\n",
    "map_SF = folium.Map([37.773972, -122.431297],tiles = \"Stamen Toner\", zoom_start=13)\n",
    "\n",
    "# Ensure you're handing it floats\n",
    "data['X'] = data['X'].astype(float)\n",
    "data['Y'] = data['Y'].astype(float)\n",
    "\n",
    "# Filter the DF for rows, then columns, then remove NaNs\n",
    "df = data[data['Category'] == 'DRIVING UNDER THE INFLUENCE'] # Reducing data size so it runs faster\n",
    "df_heat = df[['Y', 'X']] \n",
    "\n",
    "\n",
    "# Create weight column, using date\n",
    "# year\n",
    "#df_heat['Weight'] = df['Datetime'].dt.year \n",
    "# month\n",
    "#df_heat['Weight'] = df['Datetime'].dt.month \n",
    "# day\n",
    "df_heat['Weight'] = df['Datetime'].dt.day\n",
    "                    #df['Date'].str[6:10] #Can also be called directly from 'Date'\n",
    "                    \n",
    "df_heat['Weight'] = df_heat['Weight'].astype(float)\n",
    "#heat_df = heat_df.dropna(axis=0, subset=['Latitude','Longitude', 'Weight'])\n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "#Year heatmap time\n",
    "#data_heat = [[[row['Y'],row['X']] for index, row in df_heat[df_heat['Weight'] == i].iterrows()] for i in range(2013,2019)]\n",
    "#Month heatmap time\n",
    "#data_heat = [[[row['Y'],row['X']] for index, row in df_heat[df_heat['Weight'] == i].iterrows()] for i in range(1,13)]\n",
    "#Day heatmap time\n",
    "data_heat = [[[row['Y'],row['X']] for index, row in df_heat[df_heat['Weight'] == i].iterrows()] for i in range(1,32)]\n",
    "\n",
    "# Plot it on the map\n",
    "hm = plugins.HeatMapWithTime(data_heat,auto_play=True,max_opacity=0.8,index_steps=7) #index_steps = steps (days, months, years) it will jump when you plays\n",
    "hm.add_to(map_SF)\n",
    "# Display the map\n",
    "map_SF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANGLER KOMMENTAR, kommer der hvis I er enige med katagorien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Errors in the data. The importance of looking at raw (or close to raw) data.\n",
    "\n",
    "We started the course by plotting simple histogram plots that showed a lot of cool patterns. But sometimes the binning can hide imprecision, irregularity, and simple errors in the data that could be misleading. In the work we've done so far, we've already come across at least three examples of this in the SF data. \n",
    "\n",
    "1. In the hourly activity for `PROSTITUTION` something surprising is going on on Wednesday. Remind yourself [**here**](https://raw.githubusercontent.com/suneman/socialdataanalysis2020/master/files/prostitution_hourly.png), where I've highlighted the phenomenon I'm talking about.\n",
    "1. When we investigated the details of how the timestamps are recorded using jitter-plots, we saw that many more crimes were recorded e.g. on the hour, 15 minutes past the hour, and to a lesser in whole increments of 10 minutes. Crimes didn't appear to be recorded as frequently in between those round numbers. Remind yourself [**here**](https://raw.githubusercontent.com/suneman/socialdataanalysis2020/master/files/jitter_plot.png), where I've highlighted the phenomenon I'm talking about.\n",
    "1. And finally, today we saw that the Hall of Justice seemed to be an unlikely hotspot for sex offences. Remind yourself [**here**](https://raw.githubusercontent.com/suneman/socialdataanalysis2020/master/files/crime_hot_spot.png).\n",
    "\n",
    "Exercise: Data errors. The data errors we discovered above become difficult to notice when we aggregate data (and when we calculate mean values, as well as statistics more generally). Thus, when we visualize, errors become difficult to notice when when we bin the data. We explore this process in the exercise below.\n",
    "\n",
    "This last exercise has two parts.\n",
    "* In each of the three examples above, describe in your own words how the data-errors I call attention to above can bias the binned versions of the data. Also briefly mention how not noticing these errors can result in misconceptions about the underlying patterns of what's going on in San Francisco (and our modeling).\n",
    "* (Optional) Find your own example of human noise in the data and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
